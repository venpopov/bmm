---
title: "The Memory Measurement Model (`m3`)"
output: bookdown::html_document2
author: 
  - Gidon Frischkorn
  - Chenyu Li
  - Isabel Courage
bibliography: REFERENCES.bib
header-includes:
  - \usepackage{amsmath}
vignette: >
  %\VignetteIndexEntry{The Memory Measurement Model (`m3`)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
pkgdown:
  as_is: true
---

```{=html}
<style type="text/css">
div.main-container {
max-width: 800px !important;
}

p {
margin-top: 1.5em ;
margin-bottom: 1.5em ;
}
.author{
    display: none;
}
</style>
```
```{r, include = FALSE}
options(crayon.enabled = TRUE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "jpeg",
  dpi = 100,
  fig.asp = 0.8,
  fig.width = 5,
  out.width = "80%",
  fig.align = "center"
)
fansi::set_knit_hooks(knitr::knit_hooks, which = c("output","message","error"))
```

# Introduction to the model

The memory measurement model (`m3`) is a computational measurement model for working memory tasks with categorical responses. Such materials include letters, words, objects, and digits. The only prerequisite for using the `m3` is that the responses can be grouped into separate response categories. For each of these response categories the model predicts the frequency of selecting one item of a category as a function of continuous activation strengths. Most commonly the `m3` distinguishes two different activation dimensions:

1.  Memory strength of individual elements or items, often labelled item memory
2.  Memory strength of relations, relying on temporary bindings, often labelled binding or source memory

The `m3` also allows to include additional hypothetical processes that might increase or diminishing these activation strengths due to experimental manipulations, such as manipulating encoding time or distraction.

## Basic assumptions of the `m3`

The `m3` builds on two general assumptions:

1.  Memory recall represents a competitive selection from a set of response candidates
2.  Selection from the candidate set is a function of the relative activation of each candidate representation at test

The set of response candidates can either a) follow naturally from the stimulus material, for example all digits from 1 to 9 or all letters from the alphabet, b) be given in the experimental procedure implementing recall as the selection from `n` response alternatives (n-AFC), or c) be constructed by the individual performing the task. Generally, we recommend focusing on the first and the second use case, as these provide the simpler implementations of the `m3`.

To simplify the use of the `m3`, we do not model the activation of each response candidate, but we group the response candidates into categories that all share the same sources of activation. For example, consider a simple short term memory task (simple span task) in which subjects are asked to remember digits in their serial order and are then cued with a random serial position to retrieve the respective digit. In this task, participants can either recall the correct digit that was associated with the cued position, often labelled the item-in-position. But they could also recall any of the other digits they had to remember, often labelled items from other list positions. Finally, they could also recall a digit that was not part of the memory set at all, usually labelled not-presented items or lures. Thus, we have three categories of responses in this task: correct responses or items-in position (labelled $correct$), other list items (labelled $other$), and not presented items (labelled $npl$).

After we have decided on the relevant response categories, we need to specify which activation sources contribute towards each category. For the above simple short term memory tasks, these would most reasonably be:

$$
\begin{align} 
correct & = b + a + c\\
other & = b + a\\
npl & = b
\end{align}
$$

In this case, $b$ is the baseline activation or background noise, that can be understood as the activation of all digits, because participants know that they have to remember digits in general. The parameter $a$ is the memory strength for items, or general activation, for all items that need to be remembered in the current trial. And parameter $c$ is the memory strength for relations, or context activation, that arises from the context cue of retrieving the cued serial position.

## Choice Rules in the `m3`

The activation for each category is then translated into probabilities of recalling one item from each category with a normalization function as a choice rule. In `bmm` we have implemented two choice rules, that are different implementations of Luce's choice axiom:

1.  Simple normalization (`"simple"`)

This choice rule normalizes the absolute activation over the sum of the activation of all items:

$$
p_i = \frac{n_i \cdot A_i}{\sum^n_{j = 1} n_j \cdot A_j}
$$

In this normalization the probability $p_i$ of recalling an item from the category $i$ results from the number of candidates in the category $n_i$ times the activation of the category $A_i$ as specified in the activation formulas above. This total activation of the category $i$ then gets divided by the sum of all categories $n$ and their activation that arises as the product of the number of items in the category $n_j$ times the activation of that category $A_j$.

2.  Softmax

The `softmax` choice rule normalizes the exponentiated activation of each category over the sum of all exponentiated activations:

$$
p_i = \frac{n_i \cdot e^{A_i}}{\sum^n_{j = 1} n_j \cdot e^{A_j}}
$$

This choice rule can be interpreted as an n-alternative SDT model over the different response candidates with a Gumbel (or double-exponential) noise distribution.

The model links the response frequencies $Y$ to the probabilities $p$ using a Multinomial distribution with the total number of trials:

$$
Y \sim multinomial(p, trials)
$$

# Parametization of the `m3` in `bmm`

There are three versions of the `m3` model implemented in `bmm`:

-  the `m3` for simple span tasks (`version = "ss"`)
-  the `m3` for complex span tasks (`version = "cs"`)
-  a fully *custom* `m3` that can be adapted to any kind of task with categorical responses (`version = "custom"`).

## `m3` for simple and complex span tasks

The simple and complex span versions of the `m3` implement the activation functions outlined in Formula 1 (simple span, see also the activation formulas above) and Formula 3 (complex span) in @oberauerSimpleMeasurementModels2019. In addition to the three response categories of the simple span task, the complex span model distinguished two additional response categories: distractors in or close to the cued position ($dist_{context}$), and distractors from other or far away positions ($dist_{other}$). Thus, the `m3` for complex span tasks requires that you use distractors that could be potentially recalled.[^1]

[^1]: Traditional complex span tasks, such as the operation span or reading span task are thus not suited for the `m3`, as the distractors are different from the to-be-remembered items. For example, in the operation span task, participants are instructed to-remember a set of letters in order while judging math equations to be correct or incorrect in-between the letters. As it is unlikely, that participants would recall digits that are part of the math equations instead of letters, such tasks are not suited to distinguish processes related to the distractor processing. You could still fit the `m3` for simple span tasks to these data, this model would however not provide any insight on the way distractors are encoded and processed in working memory.

The activation equation for the `m3` for complex span tasks are as follows:

$$
\begin{align}
corr & = b + a + c \\
other & = b + a \\
dist_{c} & = b + f \cdot a + f \cdot c \\
dist_{o} & = b + f \cdot a \\
npi & = b
\end{align}
$$

For both the simple and complex span `m3` you do not have to specify the activation formulas for the different categories. You only need to provide linear or non-linear predictor formulas for the different parameters of the respective `m3` version. Using these models you can investigate how different activation sources vary over experimental manipulations or get theoretically grounded indicators of individual differences of the different activation sources.

## Custom `m3` for tasks with categorical responses

The `version = "custom"` of the `m3` allows you to specify custom activation functions for memory tasks beyond simple and complex span tasks.[^2] For example, you could use a custom `m3` to dissociate different activation sources and cognitive processes contributing to memory performance in memory updating tasks. In addition, to recalling the correct item, items from other positions, or items not presented at all during the current trial, participants can also recall *outdated items* that have been updated during the trial in such tasks. The frequency of recalling such items, can inform us of the processes contributing towards replacing the initially encoded item with the new item. One such model is discussed in @oberauerSimpleMeasurementModels2019.

[^2]: Although the `m3` was conceived as a measurement model for working memory. The `m3` framework can be generalized towards any decision paradigm involving categorical decision that could be explained by continuous activation for difference categorical responses.

For the `version = "custom"` of the `m3` you have to provide the activation functions for all response categories. Apart from that you then still provide linear prediction formulas for predicting the different parameters by experimental conditions. For details, please see the section [Fitting the `m3`](#fitting)

## Parameter Scaling the `m3`

Generally, one of the `m3` parameters has to be fixed to set the scaling of the model. In `bmm` the default is to fix the background noise parameter `b`. Thus the values of the other parameters should be interpreted relative to the background noise. The value `b` will be fixed to depends on the choice rule you choose for fitting the model. For the `simple` choice rule, `b` will be fixed to `0.1`, for the `softmax` choice rule, `b` will be fixed to `0`.

We chose two fix the background noise `b` for scaling for two reasons:

1.  Fixing the background noise is similar to fixing noise parameters in other cognitive measurement models, such as the diffusion constant `s`, or the standard deviation of the noise added to the signal or noise distribution in SDT.
2.  In `bmm` we require that all activation formulas contain at least the background noise `b`. This ensures that if there is no activation from any other source, the model predicts random guessing.

In principle, you can decide to fix another parameter for scaling. Then you need to specify which parameter should be fixed in the `bmmformula` and additionally provide a formula to predict the background noise `b`.

# <a name="fitting"></a> Fitting the M3

To fit the `m3` in `bmm`, we need to perform the same three steps as for all models fitted in `bmm`:

1.  Load data and format it to match the requirements for the `m3`
2.  Specify the `bmmformula`
3.  Create the `bmmodel` object that links the model to the variables required for fitting in the data

For the `ss` and `cs` version of the `m3` these steps are practically identical to fitting any other `bmmodel`. The `custom` version of the `m3` requires some additional `bmmformula` to specify the custom activation formulas for each response category. This will be explained below.

## The data

Let's begin by loading the `bmm` package.

```{r setup, warning = FALSE, message = FALSE}
library(bmm)
```

For this example, we will be using the data from the first experiment from @oberauerSimpleMeasurementModels2019. This data set is part of the `bmm` package as `oberauer_lewandowsky_2019_e1`

```{r}
data <- oberauer_lewandowsky_2019_e1
head(data)
```

The data contains the following variables:

-   `ID`: an integer uniquely identifying each participant in the experiment
-   `cond`: a factor distinguishing three experimental conditions that varied the type of distractors
-   `corr`, `other`, `dist`, and `npl`: The frequencies of responding with one item of the respective response categories
-   `n_corr`, `n_other`, `n_dist`, and `n_npl`: The number of response candidates in each response categories for each experimental condition

This data set already contains all the information required to fit the M3 model. But if you have a data set that is in long format and contains which `response_category` the `response` in each trial belongs to, then you need to aggregate the data and sum the number of responses for each category in each of the experimental conditions that the model should be fitted to.

## Specifying the `bmmformula` for the `m3`

After having set up the data, we specify the `bmmformula`. For the `ss` and `cs` versions of the `m3` the `bmmformula` should contain only the linear model formulas for each of the model parameters. These could look like this:

``` r
# example formula for version = ss
ss_formula <- bmf(
  c ~ 1 + cond + (1 + cond | ID),
  a ~ 1 + cond + (1 + cond | ID)
)

# example formula for version = cs
cs_formula <- bmf(
  c ~ 1 + cond + (1 + cond | ID),
  a ~ 1 + cond + (1 + cond | ID),
  f ~ 1 + (1 | ID)
)
```

For the `custom` version of the `m3` the `bmmformula` additionally needs to contain the activation formulas for each response category. This is done by using the label of each response category and predicting it by the activation function that you want to use for this category. The activation function can be any linear combination of different activation sources or non-linear function of activation sources or model parameters and other variables in the data, as exemplified in the more complex models in @oberauerSimpleMeasurementModels2019:

``` r
cat_label ~ activation_function
```

If we wanted to implement the model proposed by @oberauerSimpleMeasurementModels2019 for this data set, we need to specified four activation formulas for the response categories `corr`, `other`, `dist`, and `npl`:

```{r}
act_formulas <- bmf(
  corr ~ b + a + c,
  other ~ b + a,
  dist ~ b + d,
  npl ~ b
)
```

How you label the parameters in the activation formula is up to you, except for using underscores in the variable names and that the parameter `b` is reserved for the baseline activation or background noise that is required to be part of the activation function of each response category. Apart from that, we recommend using short labels to avoid parsing errors with special symbols.

Based on the parameter labels that we used in these activation functions, we can then specify the linear formulas for each parameter:

```{r}
par_formulas <- bmf(
  c ~ 1 + cond + (1 + cond | ID),
  a ~ 1 + cond + (1 + cond | ID),
  d ~ 1 + (1 | ID)
)
```

Then, we can combine both formulas into one by adding them using the `+` operator. Alternatively you can include all formulas in one call to `bmmformula`:

```{r}
full_formula <- act_formulas + par_formulas

# pass all formulas in one call
full_formula <- bmf(
  corr ~ b + a + c,
  other ~ b + a,
  dist ~ b + d,
  npl ~ b,
  c ~ 1 + cond + (1 + cond || ID),
  a ~ 1 + cond + (1 + cond || ID),
  d ~ 1 + (1 || ID)
)
```

## Setting up the `bmmodel` object for the `m3`

The last thing we need to do before fitting the `m3` to the data is set up the `bmmodel` object. For this we need to call the `m3` function and provide the relevant information for the model. This entails:

-   the name of the response categories
-   the number of response options in each category or the variable names that contain the number of response options in the data
-   the choice rule, if not explicitly called `bmm` will use the `softmax` as default
-   the `m3` version, if not explicitly selected `bmm` will use the `custom` version as default

Thus, a basic set up of an `m3` object looks like this:

```{r}
my_model <- m3(resp_cats = c("corr","other","dist","npl"),
               num_options = c("n_corr","n_other","n_dist","n_npl"),
               choice_rule = "simple",
               version = "custom")
```

For the `custom` version, you additionally need to provide the `links` that should be used for each of the model parameters. We also recommend that you provide `default_priors` for each model parameter Otherwise, they will be set based on the provided `links`.

The `links` ensure that model parameters stay in the correct value range. In particular for the `simple` choice rule it is essential, that all activation are positive. Thus, we recommend using `log` link functions for all parameters that represent activation sources, such as general or context activation, especially when using the `simple` choice rule.

The `default_priors` are less critical, but still, we recommend that you consider what parameter ranges are reasonable for the different model parameters and provide `default_priors` for all of them. For detailed information on priors in `bmm` please see the `vignette("extract_info")`. In short, you can provide priors for intercepts as `main` and for effects as `effects`.

Setting up a `m3` object including these info looks like this:

```{r}
my_links <- list(
  c = "log", a = "log", d = "log"
)

my_priors <- list(
  c = list(main = "normal(2, 0.5)", effects = "normal(0, 0.5)"),
  a = list(main = "normal(0, 0.5)", effects = "normal(0, 0.5)"),
  d = list(main = "normal(0, 0.5)", effects = "normal(0, 0.5)")
)

my_model <- m3(resp_cats = c("corr","other","dist","npl"),
               num_options = c("n_corr","n_other","n_dist","n_npl"),
               choice_rule = "simple",
               links = my_links,
               default_priors = my_priors)
```

## Running `bmm` to estimate parameters

After having set up the `data`, the `bmmformula`, and the `bmmodel`, we can pass all information to `bmm` to fit the model:

```{r message=FALSE, warning=FALSE, echo=TRUE, results = 'hide'}
m3_fit <- bmm(
  formula = full_formula,
  data = data,
  model = my_model,
  sample_prior = "yes",
  cores = 4,
  init = 0,
  warmup = 2000, iter = 2000 + 5000,
  backend = 'cmdstanr'
)
```

Running this model takes about 20 to 40 seconds (depending on the speed of your computer). Using the `bmmfit` object we can have a quick look at the summary of the fitted model:

```{r, comment=NA}
summary(m3_fit)
```

First, we can have a look at the estimated regression coefficients. The first thing we should check is if the sampling converged, this is indicated by all `Rhat` values being close to one. If you want to do more inspection of the sampling, you can check out the functionality implemented in `brms`to do this.

The parameter estimates for `c`, `a`, and `d` are estimated using a `log` link function, so we have to transform these back to the native scale using the `exp` function:

```{r}
fixedFX <- brms::fixef(m3_fit)

# print posterior means for the c parameter
exp(fixedFX[startsWith(rownames(fixedFX),"c_"),])

# print posterior means for the a parameter
exp(fixedFX[startsWith(rownames(fixedFX),"a_"),])

# print posterior means for the d parameter
exp(fixedFX[startsWith(rownames(fixedFX),"d_"),])
```

These estimates differ from the estimates reported by @oberauerSimpleMeasurementModels2019, because we used a `log` link function, whereas in the original publication an `identity` link was used. This was done with adding truncation arguments to the priors, to ensure that all activations are positive. In principle, this is possible in `bmm`, too. However, such an estimation is less numerically stable and efficient. Therefore, we recommend using `log` links for activation parameters that should be positive.

When comparing the differences between the different conditions, the results from `bmm` converge with those in @oberauerSimpleMeasurementModels2019. The goal of this first experiment was to show a selective influence of different conditions on the `c` and `a` parameter. This replicated using the `bmm` implementation of the proposed `m3`. To evaluate the posterior differences between the conditions, we can use the `hypothesis` function from `brms`:

```{r}
post_diff <- c(
  c_newVoldR = "c_condoldreordered = 0",
  c_newVoldS = "c_condoldsame = 0",
  c_oldRVolds = "c_condoldreordered = c_condoldsame",
  a_newVoldR = "a_condoldreordered = 0",
  a_newVoldS = "a_condoldsame = 0",
  a_oldRVolds = "a_condoldreordered = a_condoldsame"
)

hyp <- brms::hypothesis(m3_fit, post_diff)
hyp
```

These hypothesis will compute the difference between the specified parameters. You can see that the `c` parameter in the condition `oldsame` differs from both the `new` and `oldreorderd` condition. In contrast the `a` parameter in the condition `oldreordered` differs from both the `new` and the `oldsame` condition. For these differences the 95% CI does not include zero as indicated by the `*` at the end of the line.

As we included a statement to also sample the priors using the `sample_prior = TRUE` option from `brms` when running the `bmm`, the `Evid.Ratio` and `Post.Prob` column given us the Bayes Factor in favor of the specified hypothesis and the posterior probability for the hypothesis being true.

# References
